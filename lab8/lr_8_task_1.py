# -*- coding: utf-8 -*-
"""LR_8_task_1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yjD6wiZBonWl9kA2ePWU10aU8824EKG7
"""

import numpy as np
import tensorflow as tf

# 1. Генерація даних
n_samples = 1000   # загальна кількість точок
batch_size = 100   # розмір батчу для навчання
num_steps = 20000  # кількість ітерацій

# Згенеруємо випадкові X і y = 2*X + 1 + шум
X_data = np.random.uniform(1, 10, (n_samples, 1)).astype(np.float32)
y_data = (2*X_data + 1 + np.random.normal(0, 2, (n_samples, 1))).astype(np.float32)

# 2. Створюємо параметри моделі
# k - коефіцієнт (нахил), b - зсув (bias)
k = tf.Variable(tf.random.normal((1, 1)), name='slope')
b = tf.Variable(tf.zeros((1,)), name='bias')


# 3. Оптимізатор
optimizer = tf.keras.optimizers.SGD(learning_rate=0.001)  # градієнтний спуск

display_step = 100  # показувати результати кожні 100 ітерацій


# 4. Навчання
for i in range(num_steps):
    # Випадковий вибір індексів для батчу
    indices = np.random.choice(n_samples, batch_size)
    X_batch, y_batch = X_data[indices], y_data[indices]

    # Відкриваємо GradientTape для автоматичного обчислення градієнтів
    with tf.GradientTape() as tape:
        y_pred = tf.matmul(X_batch, k) + b              # передбачення моделі
        loss = tf.reduce_mean((y_batch - y_pred) ** 2)  # MSE для стабільності

    # Обчислюємо градієнти відносно k і b
    grads = tape.gradient(loss, [k, b])
    # Оновлюємо параметри
    optimizer.apply_gradients(zip(grads, [k, b]))

    # Вивід результатів
    if (i + 1) % display_step == 0:
        print(f'Епоха {i+1}: loss={loss.numpy():.8f}, k={k.numpy()[0][0]:.4f}, b={b.numpy()[0]:.4f}')